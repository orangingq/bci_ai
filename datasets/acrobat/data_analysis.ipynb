{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import tifffile as tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_path(patient, img_type='all', type='train'):\n",
    "    data_dir = os.getcwd() # 'datasets/acrobat'\n",
    "    directory = os.path.join(data_dir, type)\n",
    "    ER_path = os.path.join(directory, f'{patient}_ER_{type}.tif')\n",
    "    HE_path = os.path.join(directory, f'{patient}_HE_{type}.tif')\n",
    "    HER2_path = os.path.join(directory, f'{patient}_HER2_{type}.tif')\n",
    "    KI67_path = os.path.join(directory, f'{patient}_KI67_{type}.tif')\n",
    "    PGR_path = os.path.join(directory, f'{patient}_PGR_{type}.tif')\n",
    "    paths = {}\n",
    "    if os.path.exists(ER_path):\n",
    "        paths['ER'] = ER_path\n",
    "    if os.path.exists(HE_path):\n",
    "        paths['HE'] = HE_path\n",
    "    if os.path.exists(HER2_path):\n",
    "        paths['HER2'] = HER2_path\n",
    "    if os.path.exists(KI67_path):\n",
    "        paths['KI67'] = KI67_path\n",
    "    if os.path.exists(PGR_path):\n",
    "        paths['PGR'] = PGR_path\n",
    "    if img_type == 'all':\n",
    "        return paths\n",
    "    else:\n",
    "        return paths[img_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = image_path(42)\n",
    "\n",
    "# Open the large TIFF file\n",
    "for img_type, path in paths.items():\n",
    "    with tiff.TiffFile(path) as tif:\n",
    "        for page in tif.pages:\n",
    "            print(f\"[{img_type}]\\tshape : {page.shape}, dtype: {page.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "def load_image(filepath, resolution='medium'):\n",
    "    page=2\n",
    "    if resolution == 'low':\n",
    "        page=-2\n",
    "    elif resolution == 'high':\n",
    "        page=0\n",
    "    with tiff.TiffFile(filepath) as tif:\n",
    "        image = tif.pages[page].asarray() # medium resolution\n",
    "    return image # (H, W, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image(image_path(42, 'ER'))\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess \n",
    "## 1. Crop the tissue part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to check if a window contains tissue\n",
    "def is_tissue_present(patch, threshold_value=200, min_tissue_ratio=0.05):\n",
    "\n",
    "    # Convert the patch to grayscale\n",
    "    gray_patch = cv2.cvtColor(patch, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Apply binary thresholding\n",
    "    _, binary_patch = cv2.threshold(gray_patch, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Calculate the percentage of non-white pixels (tissue)\n",
    "    non_background_pixels = np.sum(binary_patch == 0)  # Count black pixels (tissue)\n",
    "    total_pixels = patch.shape[0] * patch.shape[1]\n",
    "    \n",
    "    tissue_ratio = non_background_pixels / total_pixels\n",
    "    \n",
    "    return tissue_ratio > min_tissue_ratio\n",
    "\n",
    "# Function to slide a window over the image\n",
    "def sliding_window(image, window_size, step_size, padding=100):\n",
    "    assert padding < image.shape[0] - window_size[1] -padding + 1 and padding < image.shape[1] - window_size[0] -padding + 1, f\"Padding is too large : {padding} < {image.shape[0] - window_size[1] -padding + 1}, {padding} < {image.shape[1] - window_size[0] -padding + 1}\"\n",
    "    for y in range(padding, image.shape[0] - window_size[1] -padding + 1, step_size):\n",
    "        for x in range(padding, image.shape[1] - window_size[0] -padding + 1, step_size):\n",
    "            patch = image[y:y + window_size[1], x:x + window_size[0]]\n",
    "            yield (x, y, patch)\n",
    "\n",
    "# Function to find the bounding box of the tissue region\n",
    "def find_tissue_bounding_box(image, window_size=(512, 512), step_size=256, threshold_value=200, min_tissue_ratio=0.05, padding=100):\n",
    "    tissue_coords = []\n",
    "\n",
    "    # Slide window over the image\n",
    "    for (x, y, patch) in sliding_window(image, window_size, step_size, padding):\n",
    "        if is_tissue_present(patch, threshold_value, min_tissue_ratio):\n",
    "            tissue_coords.append((x, y, x + window_size[0], y + window_size[1]))\n",
    "\n",
    "    if not tissue_coords:\n",
    "        # print(\"No tissue found.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Calculate the bounding box that covers all tissue regions\n",
    "    x_min = min([x1 for (x1, y1, x2, y2) in tissue_coords])\n",
    "    y_min = min([y1 for (x1, y1, x2, y2) in tissue_coords])\n",
    "    x_max = max([x2 for (x1, y1, x2, y2) in tissue_coords])\n",
    "    y_max = max([y2 for (x1, y1, x2, y2) in tissue_coords])\n",
    "\n",
    "    return x_min, y_min, x_max - x_min, y_max - y_min\n",
    "\n",
    "# Full process\n",
    "def process_image(image, threshold_value=200, min_tissue_ratio='auto', padding=100):\n",
    "    padding = max(padding, 1)\n",
    "    if image.shape[0] < image.shape[1]:\n",
    "        window_size = (image.shape[0]//2 - 2*padding, image.shape[0] - 2*padding) # (x, y)\n",
    "        step_size = image.shape[0]//8\n",
    "    else:\n",
    "        window_size = (image.shape[1] - 2*padding, image.shape[1]//2 - 2*padding)\n",
    "        step_size = image.shape[1]//8\n",
    "\n",
    "    if min_tissue_ratio == 'auto': # Calculate min_tissue_ratio\n",
    "        gray_patch = cv2.cvtColor(image[padding:-padding, padding:-padding, :], cv2.COLOR_RGB2GRAY)\n",
    "        _, binary_patch = cv2.threshold(gray_patch, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "        non_background_pixels = np.sum(binary_patch == 0)  # Count black pixels (tissue)\n",
    "        total_pixels = binary_patch.shape[0] * binary_patch.shape[1]\n",
    "        min_tissue_ratio = non_background_pixels / total_pixels\n",
    "        # print(f\"Set min_tissue_ratio: {min_tissue_ratio}\")\n",
    "\n",
    "    x, y, w, h = find_tissue_bounding_box(image, window_size, step_size, threshold_value, min_tissue_ratio, padding)\n",
    "    \n",
    "    if x is None:\n",
    "        while threshold_value > 0 and x is None:\n",
    "            threshold_value -= 10\n",
    "            min_tissue_ratio -= 0.01\n",
    "            # print('threshold_value : ', threshold_value)\n",
    "            x, y, w, h = find_tissue_bounding_box(image, window_size, step_size, threshold_value, min_tissue_ratio, padding)\n",
    "    \n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to crop the ROI\n",
    "def crop_roi(image, x, y, w, h):\n",
    "    return image[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original image\n",
    "img = load_image(image_path(42, 'ER'))\n",
    "plt.title(\"Original Image (x0.1 downsampled)\")\n",
    "plt.imshow(img[::10, ::10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Draw a rectangle around the ROI on the original image\n",
    "x, y, w, h = process_image(img, min_tissue_ratio='auto')\n",
    "if x is not None:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img[::10, ::10, :])\n",
    "    rect = Rectangle((x//10, y//10), w//10, h//10, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.title(\"Original Image with ROI (x0.1 downsampled)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed image (tissue only)\n",
    "x, y, w, h = process_image(img)\n",
    "cropped_image = crop_roi(img, x, y, w, h)\n",
    "if cropped_image is not None:\n",
    "    plt.title(\"Cropped Tissue (x0.1 downsampled)\")\n",
    "    plt.imshow(cropped_image[::10, ::10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess every image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(image, x, y, w, h, title=\"Original Image with ROI\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    rect = Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roi_for_patient(patient, resolution='medium', padding=100):\n",
    "    paths = image_path(patient)\n",
    "    num_images = len(paths)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, num_images))\n",
    "    for ax, (img_type, path) in zip(axes, paths.items()):\n",
    "        img = load_image(path, resolution)\n",
    "        ax.imshow(img.copy())\n",
    "        x, y, w, h = process_image(img, padding=padding)\n",
    "        if x is not None:\n",
    "            rect = Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "        ax.set_title(f\"{img_type} Image with ROI\")\n",
    "        # ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_roi_for_patient(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi_for_patient(patient, resolution='medium', padding=100, save=False):\n",
    "    paths = image_path(patient)\n",
    "    num_images = len(paths)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, num_images))\n",
    "    save_to = os.path.join(os.path.dirname(list(paths.values())[0]), 'roi')\n",
    "    if not os.path.exists(save_to):\n",
    "        os.makedirs(save_to)\n",
    "    for ax, (img_type, path) in zip(axes, paths.items()):\n",
    "        img = load_image(path, resolution)\n",
    "        x, y, w, h = process_image(img, padding=padding)\n",
    "        if x is not None:\n",
    "            roi = crop_roi(img, x, y, w, h)\n",
    "            if save:\n",
    "                Image.fromarray(roi).save(os.path.join(save_to, f'{patient}_{img_type}_ROI.png'))\n",
    "            ax.imshow(roi)\n",
    "        ax.set_title(f\"Cropped {img_type} Image\")\n",
    "        # ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_roi_for_patient(42, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient in range(0, 147, 10):\n",
    "    print(f\"Patient {patient}\")\n",
    "    draw_roi_for_patient(patient, padding=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Place WSI files  as `WSI\\[DATASET_NAME]\\[CATEGORY_NAME]\\[SLIDE_FOLDER_NAME] (optional)\\SLIDE_NAME.tif`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "base_dir = 'train/WSI'\n",
    "for file in os.listdir(base_dir):\n",
    "    if file.endswith('.tif'):\n",
    "        patient = file.split('_')[0]\n",
    "        random_class = int(patient)%4\n",
    "        # move the file to WSI/acrobat/random_class\n",
    "        destination_dir = os.path.join(base_dir, 'acrobat', str(random_class))\n",
    "        os.makedirs(destination_dir, exist_ok=True)\n",
    "        shutil.move(os.path.join(base_dir, file), os.path.join(destination_dir, file))\n",
    "        print(f\"Move {file} to {destination_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read labels from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'0': '1', '101': '1', '102': 'neg', '103': '3', '105': '1', '106': 'neg', '108': '2', '109': 'neg', '110': '1', '111': '1', '112': '2', '114': '2', '115': 'neg', '117': 'neg', '121': '2', '122': '2', '123': '1', '125': '1', '126': '2', '128': '2', '12': '1', '130': '1', '131': '1', '132': '1', '133': 'neg', '134': '3', '135': '2', '136': '1', '137': '1', '13': '2', '140': '1', '141': '2', '143': '2', '144': '1', '145': '2', '146': '2', '147': '1', '15': '1', '16': '1', '20': '2', '21': 'neg', '22': 'neg', '24': '2', '25': '3', '27': '3', '28': '2', '29': '2', '2': '1', '31': '1', '32': '1', '33': '1', '34': '2', '35': '2', '36': '3', '37': '2', '40': 'neg', '42': '2', '46': '3', '48': '1', '49': '1', '4': '1', '51': '1', '52': 'neg', '53': '1', '54': '2', '55': '1', '56': '2', '58': '1', '5': '1', '60': '1', '61': '1', '62': '1', '65': '1', '66': '1', '67': '1', '68': '1', '6': '1', '70': '1', '71': '2', '72': '1', '74': '1', '77': '1', '78': '1', '7': '1', '80': '1', '81': '2', '82': '1', '83': '1', '85': '1', '87': '1', '88': '1', '90': '1', '91': '1', '92': 'neg', '93': '2', '95': '1', '97': '2', '98': 'neg', '99': '1', '148': '1', '150': '1', '151': '2', '152': '1', '154': 'neg', '155': '3', '159': '1', '160': '2', '161': '1', '163': '1', '165': '1', '166': '1', '168': '1', '169': 'neg', '170': '1', '172': '1', '174': '1', '175': '1', '176': '1', '178': '2', '181': '2', '182': 'neg', '185': '1', '186': '1', '188': 'neg', '189': '3', '191': '2', '192': '1', '193': '1', '194': '1', '195': '1', '197': '1', '199': '1', '201': '1', '202': 'neg', '208': '1', '210': '1', '211': '1', '212': '2', '213': '1', '214': '2', '215': '1', '217': 'neg', '219': '1', '220': '1', '222': '1', '224': '2', '225': '3', '226': '3', '228': '2', '229': '1', '230': '3', '234': '1', '237': '1', '239': '1', '240': '2', '242': '1', '243': '3', '244': '2', '245': '1', '246': '2', '247': '1', '248': '2', '252': '1', '256': '1', '257': '2', '258': '1', '259': 'neg', '261': '1', '262': '2', '264': '1', '265': '1', '266': '1', '268': '1', '269': '1', '272': '2', '273': '1', '274': '1', '276': 'neg', '278': '1', '279': '1', '280': '1', '281': '1', '284': 'neg', '286': '1', '287': '1', '289': '1', '290': 'neg', '291': '2', '294': 'neg', '296': '1', '299': '1', '301': '3', '302': 'neg', '304': '1', '305': '1', '306': '2', '309': 'neg', '310': '3', '311': '1', '313': '1', '315': '1', '321': '1', '322': '3', '323': '3', '324': '1', '325': 'neg', '327': '2', '328': '1', '329': 'neg', '330': '1', '331': '1', '332': '2', '333': '1', '334': '3', '335': '1', '336': '1', '337': 'neg', '339': 'neg', '340': '2', '342': 'neg', '343': '1', '344': '1', '345': '1', '346': '1', '347': '1', '348': '1', '349': '1', '352': '1', '354': '2', '355': '1', '356': '2', '359': '2', '363': '1', '365': '2', '367': '1', '372': '2', '375': '1', '378': '2', '379': '2', '381': '1', '383': '3', '384': '1', '385': 'neg', '386': '1', '387': '2', '389': '1', '390': '1', '392': 'neg', '393': '1', '397': 'neg', '399': '1', '400': '1', '401': 'neg', '402': '2', '403': 'neg', '404': '2', '406': '2', '407': '3', '411': 'neg', '412': '1', '413': '1', '414': '3', '415': '2', '418': '1', '419': '3', '421': '1', '422': '2', '423': '1', '424': '1', '425': '2', '426': '1', '427': '1', '429': 'neg', '432': '1', '433': '1', '435': '1', '436': '2', '438': '2', '439': '1', '441': 'neg', '443': '2', '445': '2', '448': '3', '450': '1', '452': '2', '454': '1', '455': '2', '457': '2', '458': 'neg', '459': '2', '460': '2', '463': 'neg', '464': '1', '465': 'neg', '467': '1', '470': '1', '472': '1', '473': '1', '474': '1', '478': '3', '479': '2', '481': 'neg', '484': '2', '488': '2', '490': '2', '491': '3', '492': '2', '493': '1', '494': '1', '495': '2', '497': 'neg', '499': '1', '500': 'neg', '502': '3', '503': '1', '504': '2', '508': 'neg', '511': '2', '513': 'neg', '514': '1', '517': 'neg', '518': '2', '519': '2', '520': '2', '521': '1', '524': '2', '526': '2', '527': '2', '528': '2', '530': 'neg', '531': '1', '533': '1', '535': '1', '537': '1', '538': 'neg', '539': '1', '540': 'neg', '541': '2', '542': '1', '543': '1', '545': '1', '547': 'neg', '548': '3', '549': '1', '550': '1', '551': '1', '552': '3', '553': '2', '557': '1', '558': 'neg', '559': 'neg', '560': 'neg', '561': '1', '566': '1', '568': '1', '569': 'neg', '570': 'neg', '571': '1', '572': '1', '574': 'neg', '576': '1', '577': '3', '578': '2', '579': '1', '582': 'neg', '583': '1', '585': 'neg', '588': '2', '589': '1', '591': '1', '594': 'neg', '595': '2', '596': '2', '597': '2', '601': '1', '602': '1', '604': '3', '605': '2', '606': '1', '607': '1', '608': '3', '609': '2', '610': '2', '611': '1', '614': 'neg', '615': '3', '616': '2', '617': '2', '618': '1', '619': '2', '621': '2', '623': '2', '627': '1', '628': '1', '630': '1', '631': '1', '632': '1', '634': '1', '635': '2', '640': '2', '642': '1', '643': '1', '644': '2', '646': '1', '647': '1', '648': '1', '649': '1', '651': 'neg', '653': 'neg', '657': '1', '658': '3', '661': '2', '662': '1', '663': 'neg', '664': 'neg', '665': '1', '666': 'neg', '668': '2', '669': '1', '671': 'neg', '674': '1', '677': '1', '679': 'neg', '680': 'neg', '681': '1', '682': '1', '684': '1', '685': '1', '689': 'neg', '690': '3', '691': '1', '692': '1', '693': 'neg', '695': '1', '696': 'neg', '698': 'neg', '699': '2', '700': 'neg', '703': '2', '705': '1', '707': '1', '709': 'neg', '710': '1', '711': '1', '712': '1', '714': '2', '715': '3', '717': '1', '718': '1', '721': 'neg', '724': 'neg', '726': 'neg', '727': '1', '733': 'neg', '737': 'neg', '739': '2', '740': 'neg', '742': 'neg', '743': '1', '745': 'neg', '746': '2', '747': '1', '748': '2', '749': '3'}, 'valid': {'23': '1', '82': 'neg', '38': 'neg', '39': 'neg', '33': '1', '52': 'neg', '9': '1', '4': '1', '87': '2', '91': 'neg', '77': '1', '43': '1', '59': '2', '41': '1'}, 'test': {'250': '1', '241': '1', '135': '1', '107': '1', '184': '3', '167': '1', '119': '1', '122': '2', '199': '1', '87': '1', '214': '2', '277': '3', '25': '3', '297': '1', '292': '2', '33': '3', '299': '1', '88': 'neg', '81': '3', '248': '2', '255': '1', '169': '1', '224': '2', '193': '2', '260': '2', '27': '3', '142': '2', '89': '2', '144': '3', '233': '1', '76': '1', '71': 'neg', '231': '2', '194': '1', '223': '2', '116': '2', '104': '2', '156': '1', '213': '2', '86': '3', '192': '2', '23': '1', '163': '2', '123': '2', '117': '2', '94': '1', '17': '1', '149': '1', '95': '1', '147': '1', '162': '1', '276': '1', '121': '2', '21': '3', '16': '2', '102': '2'}}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os \n",
    "\n",
    "label_dict = {'train': {}, 'valid': {}, 'test': {}}\n",
    "current_dir = os.getcwd()  # should be '.../bci_ai/datasets/acrobat'\n",
    "with open(f'{current_dir}/train/acrobat_label.csv', 'r') as f:\n",
    "    f.readline()  # Skip the header\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        patient, dataset, label, _, _ = row\n",
    "        if label == '0' or label in [None, '', '?']: # 0: negative / 1~3: positive\n",
    "            label = 'neg'\n",
    "        elif label not in ['0', '1', '2', '3']: # label should be in ['0', '1', '2', '3']\n",
    "            raise ValueError(f\"Invalid label: {label}\")\n",
    "        \n",
    "        if dataset == 'test':\n",
    "            label_dict['test'][patient] = label\n",
    "        elif dataset == 'valid':\n",
    "            label_dict['valid'][patient] = label\n",
    "        else:\n",
    "            label_dict['train'][patient] = label\n",
    "\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make label folders : `bci_ai/datasets/acrobat/{label}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_folders = ['neg', '1', '2', '3']\n",
    "\n",
    "for folder in label_folders:\n",
    "    label_dir = os.path.join(current_dir, folder)\n",
    "    os.makedirs(label_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy IHC files to the corresponding label folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 27_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 131_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 211_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 40_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 72_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 191_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 296_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 163_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 115_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 53_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 291_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 92_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 181_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 87_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 246_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 278_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 54_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 70_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 34_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 195_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 29_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 225_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 103_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 67_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 172_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 49_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 25_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 91_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 178_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 148_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 78_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 169_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 5_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 12_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 117_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 109_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 286_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 98_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 143_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 7_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 33_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 48_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 262_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 146_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 287_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 244_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 226_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 152_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 15_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 108_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 259_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 215_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 6_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 56_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 213_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 13_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 46_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 80_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 188_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 239_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 102_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 219_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 159_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 145_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 105_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 217_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 189_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 220_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 290_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 136_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 176_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 112_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 134_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 71_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 199_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 121_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 42_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 214_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 2_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 261_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 192_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 106_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 185_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 21_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 265_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 269_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 110_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 65_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 154_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 194_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 284_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 147_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 274_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 166_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 123_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 61_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 24_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 257_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 0_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 170_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 31_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 224_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 247_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 82_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 51_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 245_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 37_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 93_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 186_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 150_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 125_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 294_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 122_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 234_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 168_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 144_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 28_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 266_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 252_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 201_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 60_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 165_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 151_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 66_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 299_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 281_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 137_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 289_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 111_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 97_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 99_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 193_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 230_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 32_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 114_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 132_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 210_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 268_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 248_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 161_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 264_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 141_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 272_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 36_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 83_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 135_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 20_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 133_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 256_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 4_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 160_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 126_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 273_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 237_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 130_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 280_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 228_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 182_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 74_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 22_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 101_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 140_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 222_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 279_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 212_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 88_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 55_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 202_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 240_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 16_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 276_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 95_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 62_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 128_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 155_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 90_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 35_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 174_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 52_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 81_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 85_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 77_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 243_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 208_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 258_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 68_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 242_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 229_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 58_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 197_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n",
      "Copied 175_HER2_train.tif to /data3/shcho/bci_ai/datasets/acrobat/3\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "dataset_type = 'train'\n",
    "from_dir = os.path.join(current_dir, dataset_type, 'WSI')\n",
    "for file in os.listdir(from_dir):\n",
    "    file_number, img_type, _ = file.split('_')\n",
    "    if file.endswith('.tif') and img_type == 'HER2':\n",
    "        assert file_number in label_dict[dataset_type], f\"Patient {file_number} not found in the label dictionary\"\n",
    "        label = label_dict[dataset_type][file_number]\n",
    "        to_dir = os.path.join(current_dir, label)\n",
    "        shutil.copy(os.path.join(from_dir, file), to_dir)\n",
    "        print(f\"Copied {file} to {label_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bci_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
